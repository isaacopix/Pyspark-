# -*- coding: utf-8 -*-
"""Pyspark dictionary transformation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bv5X43DPWs_zTJdApVPYUz4VITQ5CUmQ

Pyspark implementation of a group_sort (input _path) method that reads data from the jobs.csv file and returns a dictionary in which the keys are jobs and the values are counts of how many times each job appears within the dataset. The dictionary is ordered by count (in ascending order), then job (in ascending order from A to Z). The group_sort (input _path) method takes one argument: input_path - a path to the CSV file containing the data.
Available packages/libraries
• Python 3.8 and all of its built-in packages
• Spark version 3.1.1
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

class SparkTask:
    def __init__(self, spark_session):
        self.job_counts_dict = None
        self.spark = spark_session

    def group_sort(self, input_path):
        # Read data
        df = self.spark.read.option("header", "true").csv(input_path)

        # Group by job and count
        counts = df.groupBy('job').count().withColumnRenamed('count', 'job_count')

        # Sort by count then by job title
        counts = counts.orderBy(col('job_count').asc(), col('job').asc())

        # Pass the results to a dictionary
        self.job_counts_dict = dict((row['job'], row['job_count']) for row in counts.collect())
        return self.job_counts_dict

# Define the dict_sort function correctly
def dict_sort(input_path):
    # Initialize a Spark session
    spark = SparkSession.builder \
        .appName("Jobs") \
        .getOrCreate()

    # Use the SparkTask class to perform grouping and sorting
    spark_task = SparkTask(spark)
    result = spark_task.group_sort(input_path)

    # Stop Spark session
    spark.stop()

    return result

# Example usage
data_path = 'jobs.csv'
result = dict_sort(data_path)
print(result)